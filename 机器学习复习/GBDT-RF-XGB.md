### GBDT

##### 判别模型和生成模型
联合概率分布（生成模型）；条件概率分布（判别模型）

例子：
- 判别式模型举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。
- 生成式模型举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。

##### 决策树
[决策树上](https://www.cnblogs.com/pinard/p/6050306.html)
[决策树下](https://www.cnblogs.com/pinard/p/6053344.html)
###### ID3
信息增益表示得知属性 a 的信息而使得样本集合不确定度减少的程度

如果选择一个特征后，信息增益最大（信息不确定性减少的程度最大），那么我们就选取这个特征。
[ID3 C4.5总结](https://zhuanlan.zhihu.com/p/26760551)

信息增益 = 信息熵 - 某特征下条件熵
信息增益率 = 信息增益 / 特征熵
**都是越大越好**
###### C4.5
[C4.5处理缺失值](https://blog.csdn.net/leaf_zizi/article/details/83503167)
###### CART
基尼指数，越小越好，表示模型的不纯度
**我们要明白，什么是回归树，什么是分类树。两者的区别在于样本输出，如果样本输出是离散值，那么这是一颗分类树。如果果样本输出是连续值，那么那么这是一颗回归树。**

对于CART分类树和回归树的建立和预测主要有两点不同：
- 连续值的处理方法不同
- 决策树建立后做预测的方式不同

[对于剪枝的理解](https://blog.csdn.net/zhengzhenxian/article/details/79083643)

**alpha > g(t) 说明C(T) > C(Tt),剪枝后的损失大于没剪枝的时候，所以不剪，只减alpha <= g(t)的；从最小的alpha开始剪枝可以理解为alpha越小，剪枝前后损失的变化越大。**
##### GBDT

[加法模型+前向分步算法](https://blog.csdn.net/u013597931/article/details/79874439)

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/gbdt_1.png)

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/gbdt_2.png)

###### 常见问题
1. **GBDT如何做分类算法？** GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。为了解决这个问题，主要有两个方法，一个是用**指数损失函数**，此时GBDT退化为Adaboost算法。另一种方法是用**类似于逻辑回归的对数似然损失函数**的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。**其实这里的误差就是样本𝑖对应类别𝑙的真实概率和𝑡−1轮预测概率的差值**
2. **GBDT的正则化** 主要有三种方式：1.第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为𝜈,对于前面的弱学习器的迭代𝑓𝑘(𝑥)=𝑓𝑘−1(𝑥)+ℎ𝑘(𝑥) -> 𝑓𝑘(𝑥)=𝑓𝑘−1(𝑥)+𝜈ℎ𝑘(𝑥) 𝜈 的取值范围为0<𝜈≤1。对于同样的训练集学习效果，较小的𝜈意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。2.是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。3.是对于弱学习器即CART回归树进行正则化剪枝

3. **GBDT的优缺点** **优点：**1.可以灵活处理各种类型的数据，包括连续值和离散值。2.在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。3.使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。 **缺点：**1.由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。

4. **为什么要用负梯度代替残差，直接计算不久可以？r=yi-fm-1(xi)** 简单来讲就一句话，为了可以扩展到更复杂的损失函数中。这时候你可能就有疑问了，难道不是所有的损失函数都在 y^ = y时最小吗？那可能你忘了正则项这一回事，如果只是经验风险最小化的话非常容易过拟合，所以一个合理的办法就是在每个基模型中加入正则项，所以在有正则项的情况下就不再是y^=y时损失函数最小了，所以我们需要计算损失函数的梯度，而不能直接使用分模型来拟合残差。只**不过因为在损失函数是平方误差的情况下对loss求函数梯度的结果恰好是\hat_y-y，所以我们才有了GBDT每次前向计算都是拟合残差的说法**，对于一般的损失函数都是拟合loss的函数梯度。所以“搞清残差跟误差的关系”，这两个概念之间在这里唯一的联系就是：**损失函数是平方误差的情况下对loss求函数梯度的结果恰好是残差**

##### 随机森林 Random Forest
作为Bagging代表性之一的模型---随机森林是集成学习中可以和梯度提升树GBDT分庭抗礼的算法，尤其是它可以很方便的并行训练，在如今大数据大样本的的时代很有诱惑力。

###### bagging的原理
![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/RF_1.png)

从上图可以看出，Bagging的弱学习器之间的确没有boosting那样的联系。它的特点在“随机采样”。那么什么是随机采样？

随机采样(bootsrap)就是从我们的训练集里面采集固定个数的样本，但是每采集一个样本后，都将样本放回。也就是说，之前采集到的样本在放回后有可能继续被采集到。对于我们的Bagging算法，一般会随机采集和训练集样本数m一样个数的样本。这样得到的采样集和训练集样本的个数相同，但是样本内容不同。如果我们对有m个样本训练集做T次的随机采样，，则由于随机性，T个采样集各不相同。

**注意到这和GBDT的子采样是不同的。GBDT的子采样是无放回采样，而Bagging的子采样是放回采样**

在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。对于这部分大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。

bagging对于弱学习器没有限制，这和Adaboost一样。但是最常用的一般也是决策树和神经网络。

bagging的集合策略也比较简单，对于分类问题，通常使用简单投票法，得到最多票数的类别或者类别之一为最终的模型输出。对于回归问题，通常使用简单平均法，对T个弱学习器得到的回归结果进行算术平均得到最终的模型输出。

由于Bagging算法每次都进行采样来训练模型，因此泛化能力很强，对于降低模型的方差很有作用。当然对于训练集的拟合程度就会差一些，也就是模型的偏倚会大一些。

###### 随机森林
首先，RF使用了CART决策树作为弱学习器，这让我们想到了梯度提升树GBDT。第二，在使用决策树的基础上，RF对决策树的建立做了改进，对于普通的决策树，我们会在节点上所有的n个样本特征中选择一个最优的特征来做决策树的左右子树划分，但是RF通过随机选择节点上的一部分样本特征，这个数字小于n，假设为𝑛𝑠𝑢𝑏，然后在这些随机选择的𝑛𝑠𝑢𝑏个样本特征中，选择一个最优的特征来做决策树的左右子树划分。这样进一步增强了模型的泛化能力。　

如果𝑛𝑠𝑢𝑏=𝑛，则此时RF的CART决策树和普通的CART决策树没有区别。𝑛𝑠𝑢𝑏越小，则模型约健壮，当然此时对于训练集的拟合程度会变差。也就是说𝑛𝑠𝑢𝑏越小，模型的方差会减小，但是偏倚会增大。在实际案例中，一般会通过交叉验证调参获取一个合适的𝑛𝑠𝑢𝑏的值。

除了上面两点，RF和普通的bagging算法没有什么不同， 下面简单总结下RF的算法。

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/RF_2.png)

在构建决策树的时候，**RF的每棵决策树都最大可能的进行生长而不进行剪枝**；在对预测输出进行结合时，RF通常对分类问题使用简单投票法，回归任务使用简单平均法。

RF的重要特性是**不用对其进行交叉验证或者使用一个独立的测试集获得无偏估计**，它可以在内部进行评估，也就是说在生成的过程中可以对误差进行无偏估计，由于每个基学习器只使用了训练集中约63.2%的样本，剩下约36.8%的样本可用做验证集来对其泛化性能进行“包外估计”。

**随机森林的优缺点** **优点**：1.训练可以高度并行化，对于大数据时代的大样本训练速度有优势。个人觉得这是的最主要的优点。2.由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。3. 在训练后，可以给出各个特征对于输出的重要性4.由于采用了随机采样，训练出的模型的方差小，泛化能力强。[利用RF对特征重要性评估](https://blog.csdn.net/zjuPeco/article/details/77371645) 5.对部分特征缺失不敏感。**缺点**：1.在某些噪音比较大的样本集上，RF模型容易陷入过拟合。2.取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。

##### XGBoost

[XGB总结及公式推导](https://www.cnblogs.com/pinard/p/10979808.html)

XGboost 的正则化剪枝操作：
![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/XGB_1.png)

这个公式形式上跟ID3算法、CART算法是一致的，都是用分裂后的某种值减去分裂前的某种值，从而得到增益。为了限制树的生长，我们可以加入阈值，**当增益大于阈值时才让节点分裂，上式中的gamma即阈值**，它是正则项里叶子节点数T的系数，所以**xgboost在优化目标函数的同时相当于做了预剪枝**。另外，上式中还有一个系数lambda，是正则项里leaf score的L2模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性。
