### GBDT

##### 判别模型和生成模型
联合概率分布（生成模型）；条件概率分布（判别模型）

例子：
- 判别式模型举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。
- 生成式模型举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。

##### 决策树
[决策树上](https://www.cnblogs.com/pinard/p/6050306.html)
[决策树下](https://www.cnblogs.com/pinard/p/6053344.html)
###### ID3
信息增益表示得知属性 a 的信息而使得样本集合不确定度减少的程度

如果选择一个特征后，信息增益最大（信息不确定性减少的程度最大），那么我们就选取这个特征。
[ID3 C4.5总结](https://zhuanlan.zhihu.com/p/26760551)

信息增益 = 信息熵 - 某特征下条件熵
信息增益率 = 信息增益 / 特征熵
**都是越大越好**
###### C4.5
[C4.5处理缺失值](https://blog.csdn.net/leaf_zizi/article/details/83503167)
###### CART
基尼指数，越小越好，表示模型的不纯度
**我们要明白，什么是回归树，什么是分类树。两者的区别在于样本输出，如果样本输出是离散值，那么这是一颗分类树。如果果样本输出是连续值，那么那么这是一颗回归树。**

对于CART分类树和回归树的建立和预测主要有两点不同：
- 连续值的处理方法不同
- 决策树建立后做预测的方式不同

[对于剪枝的理解](https://blog.csdn.net/zhengzhenxian/article/details/79083643)

**alpha > g(t) 说明C(T) > C(Tt),剪枝后的损失大于没剪枝的时候，所以不剪，只减alpha <= g(t)的；从最小的alpha开始剪枝可以理解为alpha越小，剪枝前后损失的变化越大。**
##### GBDT

[加法模型+前向分步算法](https://blog.csdn.net/u013597931/article/details/79874439)
