### GBDT

##### 判别模型和生成模型
联合概率分布（生成模型）；条件概率分布（判别模型）

例子：
- 判别式模型举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。
- 生成式模型举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。

##### 决策树
[决策树上](https://www.cnblogs.com/pinard/p/6050306.html)
[决策树下](https://www.cnblogs.com/pinard/p/6053344.html)
###### ID3
信息增益表示得知属性 a 的信息而使得样本集合不确定度减少的程度

如果选择一个特征后，信息增益最大（信息不确定性减少的程度最大），那么我们就选取这个特征。
[ID3 C4.5总结](https://zhuanlan.zhihu.com/p/26760551)

信息增益 = 信息熵 - 某特征下条件熵
信息增益率 = 信息增益 / 特征熵
**都是越大越好**
###### C4.5
[C4.5处理缺失值](https://blog.csdn.net/leaf_zizi/article/details/83503167)
###### CART
基尼指数，越小越好，表示模型的不纯度
**我们要明白，什么是回归树，什么是分类树。两者的区别在于样本输出，如果样本输出是离散值，那么这是一颗分类树。如果果样本输出是连续值，那么那么这是一颗回归树。**

对于CART分类树和回归树的建立和预测主要有两点不同：
- 连续值的处理方法不同
- 决策树建立后做预测的方式不同

[对于剪枝的理解](https://blog.csdn.net/zhengzhenxian/article/details/79083643)

**alpha > g(t) 说明C(T) > C(Tt),剪枝后的损失大于没剪枝的时候，所以不剪，只减alpha <= g(t)的；从最小的alpha开始剪枝可以理解为alpha越小，剪枝前后损失的变化越大。**
##### GBDT

[加法模型+前向分步算法](https://blog.csdn.net/u013597931/article/details/79874439)

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/gbdt_1.png)

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/gbdt_2.png)

###### 常见问题
1. **GBDT如何做分类算法？** GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。为了解决这个问题，主要有两个方法，一个是用**指数损失函数**，此时GBDT退化为Adaboost算法。另一种方法是用**类似于逻辑回归的对数似然损失函数**的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。**其实这里的误差就是样本𝑖对应类别𝑙的真实概率和𝑡−1轮预测概率的差值**
2. **GBDT的正则化** 主要有三种方式：1.第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为𝜈,对于前面的弱学习器的迭代𝑓𝑘(𝑥)=𝑓𝑘−1(𝑥)+ℎ𝑘(𝑥) -> 𝑓𝑘(𝑥)=𝑓𝑘−1(𝑥)+𝜈ℎ𝑘(𝑥) 𝜈 的取值范围为0<𝜈≤1。对于同样的训练集学习效果，较小的𝜈意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。2.是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。3.是对于弱学习器即CART回归树进行正则化剪枝

3. **GBDT的优缺点** **优点：**1.可以灵活处理各种类型的数据，包括连续值和离散值。2.在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。3.使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。 **缺点：**1.由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。

4. **为什么要用负梯度代替残差，直接计算不久可以？r=yi-fm-1(xi)** 简单来讲就一句话，为了可以扩展到更复杂的损失函数中。这时候你可能就有疑问了，难道不是所有的损失函数都在 y^ = y时最小吗？那可能你忘了正则项这一回事，如果只是经验风险最小化的话非常容易过拟合，所以一个合理的办法就是在每个基模型中加入正则项，所以在有正则项的情况下就不再是y^=y时损失函数最小了，所以我们需要计算损失函数的梯度，而不能直接使用分模型来拟合残差。只**不过因为在损失函数是平方误差的情况下对loss求函数梯度的结果恰好是\hat_y-y，所以我们才有了GBDT每次前向计算都是拟合残差的说法**，对于一般的损失函数都是拟合loss的函数梯度。所以“搞清残差跟误差的关系”，这两个概念之间在这里唯一的联系就是：**损失函数是平方误差的情况下对loss求函数梯度的结果恰好是残差**
