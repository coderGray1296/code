### GBDT + LR 进行CTR预测
[GBDT+LR融合方案实战](https://zhuanlan.zhihu.com/p/37522339)

[GBDT+LR算法解析及Python实现](https://www.cnblogs.com/wkang/p/9657032.html)

[GBDT训练实例及调用参数详解](https://www.cnblogs.com/pinard/p/6143927.html)

**GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。**

**GBDT有两种实现方式**

- Scikit-learn.ensemble.GradientBoostingClassifier
-

##### 为什么要做离散特征的one-hot？
1. 使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散特征通过one-hot编码映射到欧式空间，是因为，**在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的**，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。
2. **将离散型特征使用one-hot编码，确实会让特征之间的距离计算更加合理**。比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), x_2 = (2), x_3 = (3)。两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。


##### 分桶策略





##### 参数 超参数 及 超参数的优化方法
