### GBDT + LR 进行CTR预测
[GBDT+LR融合方案实战](https://zhuanlan.zhihu.com/p/37522339)

[GBDT+LR算法解析及Python实现](https://www.cnblogs.com/wkang/p/9657032.html)

[GBDT训练实例及调用参数详解](https://www.cnblogs.com/pinard/p/6143927.html)

**GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。**

**GBDT有两种实现方式**

- Scikit-learn.ensemble.GradientBoostingClassifier
- 利用lgb里的params={ 'boosting_type': 'gbdt' }参数

##### 为什么要做离散特征的one-hot？
1. 使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散特征通过one-hot编码映射到欧式空间，是因为，**在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的**，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。
2. **将离散型特征使用one-hot编码，确实会让特征之间的距离计算更加合理**。比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), x_2 = (2), x_3 = (3)。两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。


##### 分桶策略
1.离散化的常用方法是分桶

- 将所有样本在连续的数值属性 j 的取值从小到大排列 a0,a1,a2,...,aN
- 然后从小到大依次选择分桶边界 b1,b2,...,bM 。其中：M为分桶的数量，它是一个超参数，需要人工指定；每个桶的大小 bk-b(k-1) 也是一个超参数，需要人工指定
- 给定属性 j 的取值ai对其分桶：1.若ai < b1,则分桶编号是 0。分桶后的属性的取值为 0; 2.若bk<=ai<=bk+1，则分桶编号是 k。分桶后的属性取值是 k；3.若ai >= bM，则分桶编号是 M。分桶后的属性取值是 M

2.分桶的数量和边界通常需要人工指定。一般有两种方法：
- **根据业务领域的经验来指定**。如：对年收入进行分桶时，根据 2017 年全国居民人均可支配收入约为 2.6 万元，可以选择桶的数量为5。其中：
收入小于 1.3 万元（人均的 0.5 倍），则为分桶 0 。
年收入在 1.3 万元 ～5.2 万元（人均的 0.5～2 倍），则为分桶 1 。
年收入在 5.3 万元～26 万元（人均的 2 倍～10 倍），则为分桶 2 。
年收入在 26 万元～260 万元（人均的 10 倍～100 倍），则为分桶 3 。
年收入超过 260 万元，则为分桶 4 。
- **根据模型指定**。根据具体任务来训练分桶之后的数据集，通过超参数搜索来确定最优的分桶数量和分桶边界。

3.选择分桶大小时，有一些经验指导：
- **分桶大小必须足够小**，小到足够能区分数据，进行分层
- **分桶大小必须足够大，使每个桶内都有足够的样本**。如果桶内样本太少，则随机性太大，不具有统计意义上的说服力。
- 每个桶内的样本尽量**分布均匀**。

##### 参数 超参数 及 超参数的优化方法
