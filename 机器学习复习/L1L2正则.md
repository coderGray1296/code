### L1 L2正则

#### 过拟合 欠拟合
我们需要先判断模型是欠拟合，还是过拟合，才能确定下一步优化方向。

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/L%E6%AD%A3%E5%88%99_1.png)

- 模型欠拟合，即高偏差（high bias）是指模型未训练出数据集的特征，导致模型在训练集、测试集上的精度都很低。如图1左图所示。
- 模型过拟合，即高方差（high variance）是指模型训练出包含噪点在内的所有特征，导致模型在训练集的精度很高，但是应用到新数据集时，精度很低。如图1右图所示。

**偏差与方差** 偏差：描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。方差：描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/L%E6%AD%A3%E5%88%99_2.png)

**欠拟合的优化方法**

- **添加新特征** 从业务思路上构造新特征是最重要的优化措施！！这个思路对于模型效用的提升是根本性的，是源头上的突破。
- **模型优化：提升模型复杂度**
- **减少正则项权重** ![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/L%E6%AD%A3%E5%88%99_3.png)

**过拟合的优化方法**

- **获取更多的训练样本** 由于模型训练了包含噪音在内的所有特征，导致模型过拟合，通过获取更多的训练样本，可以衰减噪音权重。
- **减少特征数目** 1.特征共线性检查，利用[Pearson相关系数](https://blog.csdn.net/huangfei711/article/details/78456165)计算变量之间的线性相关性，如果自变量中属于中度以上线性相关的多个变量，只需要保留一个就可以。2.重要特征筛选，利用决策树模型，筛选出重要特征。3. 数据降维，主成分分析，保留特征变量重要差异。
- **增加正则项权重** ![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/L%E6%AD%A3%E5%88%99_4.png)

#### L1 L2 正则
正则化是用来防止模型过拟合的过程，常用的有L1正则化和L2正则化两种选项，分别通过在损失函数后加上参数向量的L1范式和L2范式的倍数来实现。其中L1范数表现为参数向量中的每个参数的绝对值之和，L2范数表现为参数向量中的每个参数的平方和的开方值。

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/L%E6%AD%A3%E5%88%99.jpg)

其中J是我们之前提过的损失函数，C是用来控制正则化程度的超参数，n是方程中特征的总数，也是方程中参数的总数，j代表每个参数。在这里，j要大于等于1，是因为我们的参数向量中第一个参数是 我们的截距，它通常是不参与正则化的。

###### L1 L2正则来源推导
可从带约束条件的优化求解和最大后验概率两种思路来推导 l1 、l2 正则化，下面将予以详细分析。

1.正则化理解之基于约束条件的最优化

对于模型权重系数 w 求解是通过最小化目标函数实现的。我们知道，模型的复杂度可用VC维来衡量。通常情况下，模型VC维与系数 w 的个数成线性关系：即 w 数量越多，VC维越大，模型越复杂。因此，为了限制模型的复杂度，很自然的思路是减少系数 w 的个数，即让 w 向量中一些元素为0或者说限制 w 中非零元素的个数。为此，我们可在原优化问题中加入一个约束条件：

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/L%E6%AD%A3%E5%88%99_5.png)

||.||0 范数表示向量中非零元素的个数。但由于该问题是一个NP问题，不易求解，为此我们需要稍微“放松”一下约束条件。为了达到近似效果，我们不严格要求某些权重 w 为0，而是要求权重 w 应接近于0，即尽量小。从而可用 l1 、 l2 范数来近似 l0 范数，即：

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/L%E6%AD%A3%E5%88%99_6.png)

使用 l2 范数时，为方便后续处理，可对 ||w||2 进行平方，此时只需调整 C 的取值即可。利用[拉格朗日算子法](https://www.zhihu.com/question/38586401)，我们可将上述带约束条件的最优化问题转换为不带约束项的优化问题，构造拉格朗日函数：

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/L%E6%AD%A3%E5%88%99_7.png)

故此，我们得到对 l1 、 l2 正则化的第一种理解：
- l1 正则化等价于在原优化目标函数中增加约束条件 ||w||1 <= C
- l2 正则化等价于在原优化目标函数中增加约束条件 ||w||2 ^2 <= C

2.正则化理解之最大后验概率估计

- l1正则化可通过假设权重 w 的先验分布为拉普拉斯分布，由最大后验概率估计导出
- l2正则化可通过假设权重 w 的先验分布为高斯分布，由最大后验概率估计导出

###### l1 l2 正则化效果分析

考虑带约束条件的优化解释，**对 l2 正则化为**：

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/L%E6%AD%A3%E5%88%99_8.png)

该问题的求解示意图如下所示：

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/L%E6%AD%A3%E5%88%99_9.png)

**对于 l1 正则化**

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/L%E6%AD%A3%E5%88%99_10.png)

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/L%E6%AD%A3%E5%88%99_11.png)

**L1正则化和L2正则化虽然都可以控制过拟合，但它们的效果并不相同。当正则化强度逐渐增大（即C逐渐变小）， 参数 的取值会逐渐变小，但L1正则化会将参数压缩为0，L2正则化只会让参数尽量小，不会取到0。**

1.**在L1正则化在逐渐加强的过程中，携带信息量小的、对模型贡献不大的特征的参数，会比携带大量信息的、对模型有巨大贡献的特征的参数更快地变成0，所以L1正则化本质是一个特征选择的过程，掌管了参数的“稀疏性”**。L1正则化越强，参数向量中就越多的参数为0，参数就越稀疏，选出来的特征就越少，以此来防止过拟合。因此，如果特征量很大，数据维度很高，我们会倾向于使用L1正则化。由于L1正则化的这个性质，逻辑回归的特征选择可以由Embedded嵌入法来完成。

2.L2正则化在加强的过程中，会尽量让每个特征对模型都有一些小的贡献，但携带信息少，对模型贡献不大的特征的参数会非常接近于0。**通常来说，如果我们的主要目的只是为了防止过拟合，选择L2正则化就足够了。但是如果选择L2正则化后还是过拟合，模型在未知数据集上的效果表现很差，就可以考虑L1正则化。**

#### LR实战中的L正则应用
[scikit-learn逻辑回归类库使用小结](https://www.cnblogs.com/pinard/p/6035872.html)

#### 损失函数/目标函数的优化算法汇总
![avatar](https://github.com/coderGray1296/code/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/L%E6%AD%A3%E5%88%99_11.png)

[最优化算法汇总](https://zhuanlan.zhihu.com/p/42689565)
