### 深度学习常见问题

###### 描述前向传播和反向传播过程
[前向反向传播过程](https://www.cnblogs.com/charlotte77/p/5629865.html)

###### gdbt和rf哪个树更深？

RF更深，原因可以从两个角度来解释
- RF保证每棵树都是最大程度的生长，而GBDT要拟合残差，保留一定的提升空间，所以不用使用fully-grown tree 防止没得提升。
- RF的随机样本及随机特征保证了泛化能力，不会容易过拟合，而GBDT则需要后剪枝

###### 常见的损失函数有哪些？

[损失函数总结](https://zhuanlan.zhihu.com/p/58883095)

###### 梯度消失、梯度爆炸产生的原因，及解决办法

梯度消失与梯度爆炸其实是一种情况，看接下来的文章就知道了。两种情况下梯度消失经常出现，一是在**深层网络**中，二是采用了**不合适的损失函数**，比如sigmoid。梯度爆炸一般出现在深层网络和**权值初始化值太大**的情况下，下面分别从这两个角度分析梯度消失和爆炸的原因。

1.深层网络角度：根据链式求导法则会有对激活函数求导部分乘网络权重，如果网络的层数很深同时根据初始化的w网络参数，此部分大于1就会以指数的形式增加，最终梯度爆炸；如果小于1就会以指数的形式衰减，最终梯度消失。因此，**梯度消失、爆炸，其根本原因在于反向传播训练法则**，属于先天不足

2.激活函数角度：计算权值更新信息的时候需要计算前层偏导信息，因此如果激活函数选择不合适，比如使用sigmoid梯度消失就会很明显了，如果使用sigmoid作为损失函数，其梯度是不可能超过0.25的，这样经过链式求导之后，很容易发生梯度消失；同理，tanh作为激活函数，tanh比sigmoid要好一些，但是它的导数仍然是小于1的。

**梯度消失、爆炸的解决方案**

1.**预训练+微调** 此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。Hinton在训练深度信念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。

2.**梯度剪切、正则** 梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。另外一种解决梯度爆炸的手段是采用权重正则化（weithts regularization）比较常见的是l1正则，和l2正则 Loss=(y−WTx)2+α∣∣W∣∣2 其中，α是指正则项系数，因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。 **注：事实上，在深度神经网络中，往往是梯度消失出现的更多一些**。

3.**relu、leakrelu、elu等激活函数** Relu思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。先看一下relu的数学表达式：

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/DL_1.png)
![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/DL_2.png)

从上图中，我们可以很容易看出，relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。

relu的优点：1.解决了梯度消失、爆炸的问题 2.计算方便，计算速度快 缺点： 1.由于负数部分恒为0，会导致一些神经元无法激活（引出了带泄漏的relu leakrelu=max(x*k, x),k是leak系数，一般选择0.01或者0.02，或者通过学习而来）2.输出不是以0为中心的[以0为中心的激活函数对训练的影响](https://www.jianshu.com/p/a2c9f904f8e8)

4.**batchnorm** Batchnorm是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。batchnorm全名是batch normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化保证网络的稳定性。反向传播中，经过每一层的梯度会乘以该层的权重，举个简单例子：正向传播中 f2 = f1(Wx+b) 那么反向传播中 ∂f2/∂w=∂f2/∂f1 * x ,反向传播式子中有x的存在，所以x的大小影响了梯度的消失和爆炸，batchnorm就是通过对每一层的输出规范为均值和方差一致的方法，消除了x带来的放大缩小的影响，进而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从饱和区拉倒了非饱和区。[Batchnorm的解读](https://blog.csdn.net/qq_25737169/article/details/79048516)

5.[LSTM如何解决RNN带来的梯度消失问题](https://weizhixiaoyi.com/archives/491.html)

###### 为什么要进行归一化？

1.如下图所示，蓝色的圈圈图代表的是两个特征的等高线。其中左图两个特征X1和X2的区间相差非常大，X1区间是[0,2000]，X2区间是[1,5]，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。**因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛**。

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/DL_3.png)

2.**归一化有可能会提升精度**，一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）

###### 牛顿法的优点
[牛顿法和拟牛顿法](https://zhuanlan.zhihu.com/p/46536960)
**优点**：二阶收敛，收敛速度快；**缺点**：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。**特征**：1.牛顿法收敛速度为二阶，对于正定二次函数一步迭代即达最优解 2.牛顿法是局部收敛的，当初始点选择不当时，往往导致不收敛 3.牛顿法不是下降算法，当二阶海塞矩阵非正定时，不能保证产生方向是下降方向 4.二阶海塞矩阵必须可逆，否则算法进行困难，同时计算量很大。

###### 样本不均衡对于ROC曲线及PR曲线的影响？

ROC基本不受影响，因为横纵坐标分母都是 实际为正、负的样本数量，如果负样本变为原来的10倍，纵坐标不受影响，横坐标同时成倍增加，整体也保持不变。PR会影响较大

###### dropout的原理

Dropout可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如图1所示。

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/DL_4.png)

输入是x输出是y，正常的流程是：我们首先把x通过网络前向传播，然后把误差反向传播以决定如何更新参数让网络进行学习。使用Dropout之后，过程变成如下：

（1）首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图3中虚线为部分临时被删除的神经元）

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/DL_5.png)

（2） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。

（3）然后继续重复这一过程：
- 恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）
- 从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）
- 对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）

不断重复这一过程

###### one-hot编码导致特征维度过高的解决办法？

- 比如你的分类变量是中国的村镇，那你可以合并到县或者市甚至省这一级别。想办法合并到更大一级的类别
- 有些分类变量可能频次特别低，比如出现次数小于20次或者50次的那种，你把所有这种小频次的大不了也可以合并起来。
- 低维嵌入 embedding
- PCA降维

###### 解释key-means聚类

通常，根据样本间的某种距离或者相似性来将样本分为不同类别，成为聚类。[key-means算法思想](https://www.cnblogs.com/chenhuabin/p/11790227.html)

###### 请解释并行和并发

- 你吃饭吃到一半，电话来了，你一直到吃完了以后才去接，这就说明你不支持并发也不支持并行。
- 你吃饭吃到一半，电话来了，你停了下来接了电话，接完后继续吃饭，这说明你支持并发。
- 你吃饭吃到一半，电话来了，你一边打电话一边吃饭，这说明你支持并行。

并发的关键是你有处理多个任务的能力，不一定要同时。并行的关键是你有同时处理多个任务的能力。所以我认为它们最关键的点就是：是否是『同时』。

###### 请解释AUC评价指标的意义

- 几何意义：AUC(Area Under Curves)指的是ROC曲线下的面积，该指标能较好的概括不平衡样本分类器的性能而成为很多数据挖掘竞赛的判定标准。由于仅有有限个样本，无论训练样本还是测试样本，因此无法获得最精确的ROC曲线，从而无法精确计算AUC。在实际计算中，使用类似微积分的方法，用梯形面积的和去近似。
- 物理意义：任取一对正、负样本，模型将正样本预测为正例的可能性 大于 将负样本预测为正例的可能性 的概率。解释起来可以说：同样的FPR下，也就是相同的横坐标下，纵坐标（TPR）越高，说明ROC曲线的高度越高（能够完全包住下面的ROC曲线），同时也就是AUC的值越大，说明模型的效果越好。

因此对于AUC而言，并不关心具体的预测结果或者标签，也不需要卡什么阈值，只要在预测结果之间排序即可。
