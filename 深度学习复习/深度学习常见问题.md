### 深度学习常见问题

###### 描述前向传播和反向传播过程
[前向反向传播过程](https://www.cnblogs.com/charlotte77/p/5629865.html)

###### gdbt和rf哪个树更深？

RF更深，原因可以从两个角度来解释
- RF保证每棵树都是最大程度的生长，而GBDT要拟合残差，保留一定的提升空间，所以不用使用fully-grown tree 防止没得提升。
- RF的随机样本及随机特征保证了泛化能力，不会容易过拟合，而GBDT则需要后剪枝

###### 常见的损失函数有哪些？

[损失函数总结](https://zhuanlan.zhihu.com/p/58883095)

###### 梯度消失、梯度爆炸产生的原因，及解决办法

梯度消失与梯度爆炸其实是一种情况，看接下来的文章就知道了。两种情况下梯度消失经常出现，一是在**深层网络**中，二是采用了**不合适的损失函数**，比如sigmoid。梯度爆炸一般出现在深层网络和**权值初始化值太大**的情况下，下面分别从这两个角度分析梯度消失和爆炸的原因。

1.深层网络角度：根据链式求导法则会有对激活函数求导部分乘网络权重，如果网络的层数很深同时根据初始化的w网络参数，此部分大于1就会以指数的形式增加，最终梯度爆炸；如果小于1就会以指数的形式衰减，最终梯度消失。因此，**梯度消失、爆炸，其根本原因在于反向传播训练法则**，属于先天不足

2.激活函数角度：计算权值更新信息的时候需要计算前层偏导信息，因此如果激活函数选择不合适，比如使用sigmoid梯度消失就会很明显了，如果使用sigmoid作为损失函数，其梯度是不可能超过0.25的，这样经过链式求导之后，很容易发生梯度消失；同理，tanh作为激活函数，tanh比sigmoid要好一些，但是它的导数仍然是小于1的。

**梯度消失、爆炸的解决方案**

1.**预训练+微调** 此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。Hinton在训练深度信念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。

2.**梯度剪切、正则** 梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。另外一种解决梯度爆炸的手段是采用权重正则化（weithts regularization）比较常见的是l1正则，和l2正则 Loss=(y−WTx)2+α∣∣W∣∣2 其中，α是指正则项系数，因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。 **注：事实上，在深度神经网络中，往往是梯度消失出现的更多一些**。

3.**relu、leakrelu、elu等激活函数** Relu思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。先看一下relu的数学表达式：

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/DL_1.png)
![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/DL_2.png)

从上图中，我们可以很容易看出，relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。

relu的优点：1.解决了梯度消失、爆炸的问题 2.计算方便，计算速度快 缺点： 1.由于负数部分恒为0，会导致一些神经元无法激活（引出了带泄漏的relu leakrelu=max(x*k, x),k是leak系数，一般选择0.01或者0.02，或者通过学习而来）2.输出不是以0为中心的[以0为中心的激活函数对训练的影响](https://www.jianshu.com/p/a2c9f904f8e8)

4.**batchnorm** Batchnorm是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。batchnorm全名是batch normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化保证网络的稳定性。反向传播中，经过每一层的梯度会乘以该层的权重，举个简单例子：正向传播中 f2 = f1(Wx+b) 那么反向传播中 ∂f2/∂w=∂f2/∂f1 * x ,反向传播式子中有x的存在，所以x的大小影响了梯度的消失和爆炸，batchnorm就是通过对每一层的输出规范为均值和方差一致的方法，消除了x带来的放大缩小的影响，进而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从饱和区拉倒了非饱和区。[Batchnorm的解读](https://blog.csdn.net/qq_25737169/article/details/79048516)

5.[LSTM如何解决RNN带来的梯度消失问题](https://weizhixiaoyi.com/archives/491.html)

###### 为什么要进行归一化？

1.如下图所示，蓝色的圈圈图代表的是两个特征的等高线。其中左图两个特征X1和X2的区间相差非常大，X1区间是[0,2000]，X2区间是[1,5]，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。**因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛**。

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/DL_3.png)

2.**归一化有可能会提升精度**，一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）

###### 牛顿法的优点
[牛顿法和拟牛顿法](https://zhuanlan.zhihu.com/p/46536960)
**优点**：二阶收敛，收敛速度快；**缺点**：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。**特征**：1.牛顿法收敛速度为二阶，对于正定二次函数一步迭代即达最优解 2.牛顿法是局部收敛的，当初始点选择不当时，往往导致不收敛 3.牛顿法不是下降算法，当二阶海塞矩阵非正定时，不能保证产生方向是下降方向 4.二阶海塞矩阵必须可逆，否则算法进行困难，同时计算量很大。

###### 样本不均衡对于ROC曲线及PR曲线的影响？

ROC基本不受影响，因为横纵坐标分母都是 实际为正、负的样本数量，如果负样本变为原来的10倍，纵坐标不受影响，横坐标同时成倍增加，整体也保持不变。PR会影响较大

###### dropout的原理

Dropout可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如图1所示。

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/DL_4.png)

输入是x输出是y，正常的流程是：我们首先把x通过网络前向传播，然后把误差反向传播以决定如何更新参数让网络进行学习。使用Dropout之后，过程变成如下：

（1）首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图3中虚线为部分临时被删除的神经元）

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/DL_5.png)

（2） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。

（3）然后继续重复这一过程：
- 恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）
- 从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）
- 对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）

不断重复这一过程

###### one-hot编码导致特征维度过高的解决办法？

- 比如你的分类变量是中国的村镇，那你可以合并到县或者市甚至省这一级别。想办法合并到更大一级的类别
- 有些分类变量可能频次特别低，比如出现次数小于20次或者50次的那种，你把所有这种小频次的大不了也可以合并起来。
- 低维嵌入 embedding
- PCA降维

###### 解释key-means聚类

通常，根据样本间的某种距离或者相似性来将样本分为不同类别，成为聚类。[key-means算法思想](https://www.cnblogs.com/chenhuabin/p/11790227.html)

###### 请解释并行和并发

- 你吃饭吃到一半，电话来了，你一直到吃完了以后才去接，这就说明你不支持并发也不支持并行。
- 你吃饭吃到一半，电话来了，你停了下来接了电话，接完后继续吃饭，这说明你支持并发。
- 你吃饭吃到一半，电话来了，你一边打电话一边吃饭，这说明你支持并行。

并发的关键是你有处理多个任务的能力，不一定要同时。并行的关键是你有同时处理多个任务的能力。所以我认为它们最关键的点就是：是否是『同时』。

###### 请解释AUC评价指标的意义

- 几何意义：AUC(Area Under Curves)指的是ROC曲线下的面积，该指标能较好的概括不平衡样本分类器的性能而成为很多数据挖掘竞赛的判定标准。由于仅有有限个样本，无论训练样本还是测试样本，因此无法获得最精确的ROC曲线，从而无法精确计算AUC。在实际计算中，使用类似微积分的方法，用梯形面积的和去近似。
- 物理意义：任取一对正、负样本，模型将正样本预测为正例的可能性 大于 将负样本预测为正例的可能性 的概率。解释起来可以说：同样的FPR下，也就是相同的横坐标下，纵坐标（TPR）越高，说明ROC曲线的高度越高（能够完全包住下面的ROC曲线），同时也就是AUC的值越大，说明模型的效果越好。

因此对于AUC而言，并不关心具体的预测结果或者标签，也不需要卡什么阈值，只要在预测结果之间排序即可。

###### 如何解决dropout在训练和预测的时候产生的偏差？
Dropout 和 BatchNorm 在训练和预测时，都会遇到这个问题。因此这里一起总结一下。

**Dropout**

对于Dropout来说，是在训练过程中以一定的概率的使神经元失活，即输出为0，以提高模型的泛化能力，减少过拟合。Dropout 在训练时采用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险。**而在测试时，应该用整个训练好的模型，因此不需要dropout**。

**那么应该如何平衡训练和测试之间的差异呢？**

Dropout ，在训练时以一定的概率使神经元失活，实际上就是让对应神经元的输出为0，假设失活概率为 p ，就是这一层中的每个神经元都有p的概率失活，如下图的三层网络结构中，**如果失活概率为0.5，则平均每一次训练有3个神经元失活，所以输出层每个神经元只有3个输入，而实际测试时是不会有dropout的**，输出层每个神经元都有6个输入，这样在训练和测试时，输出层每个神经元的输入和的期望会有量级上的差异。

**因此在训练时还要对第二层的输出数据除以（1-p）之后再传给输出层神经元，作为神经元失活的补偿，以使得在训练时和测试时每一层输入有大致相同的期望**。

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/dropout_1.jpg)

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/dropout_2.jpg)

**BatchNorm**

BN，Batch Normalization，就是在深度神经网络训练过程中使得每一层神经网络的输入保持相近的分布。

**BN训练和测试时的参数是一样的嘛？**

对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。**而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全量训练数据的均值和方差，这个可以通过移动平均法求得**。对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。

**BN训练时为什么不用全量训练集的均值和方差呢？**

因为在训练的第一个完整epoch过程中是无法得到输入层之外其他层全量训练集的均值和方差，只能在前向传播过程中获取已训练batch的均值和方差。**那在一个完整epoch之后可以使用全量数据集的均值和方差嘛？**

对于BN，是对每一批数据进行归一化到一个相同的分布，而**每一批数据的均值和方差会有一定的差别，而不是用固定的值**，这个差别实际上也能够增加模型的鲁棒性，也会在一定程度上减少过拟合。

但是一批数据和全量数据的均值和方差相差太多，又无法较好地代表训练集的分布，因此，BN一般要求将训练集完全打乱，并用一个较大的batch值，去缩小与全量数据的差别。

###### Internal Covariate Shift 及 Normalization总结
**Internal Covariate Shift**
```
深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。
Google 将这一现象总结为 Internal Covariate Shift，简称 ICS.
所以ICS是：将每一层的输入作为一个分布看待，由于底层的参数随着训练更新，导致相同的输入分布得到的输出分布改变了。
```
而机器学习中有个很重要的假设：**IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障**。那么，细化到神经网络的每一层间，每轮训练时分布都是不一致，那么相对的训练效果就得不到保障，所以称为层间的covariate shift。

可以看到，随着网络层数的加深，输入分布经过多次线性非线性变换，已经被改变了，但是它对应的标签，如分类，还是一致的，即使条件概率一致，边缘概率不同。因此，每个神经元的输入数据不再是“独立同分布”，导致了以下问题：
- 上层参数需要不断适应新的输入数据分布，降低学习速度。
- 下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。
- 每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。

**白化(Whitening)**

白化（Whitening）是机器学习里面常用的一种规范化数据分布的方法，主要是PCA白化与ZCA白化。白化是对输入数据分布进行变换，进而达到以下两个目的：
- 使得输入特征分布具有相同的均值与方差，其中PCA白化保证了所有特征分布均值为0，方差为1；而ZCA白化则保证了所有特征分布均值为0，方差相同
- 去除特征之间的相关性。

**但是白化的计算成本过高，需要在每一轮每一层都进行白化操作；同时白化让每一层的分布均为均值为0，方差相同的分布，而没有学习到数据的分布特征，导致网络的表达能力受限，因此提出了normalization操作，既能够简化计算过程；又能够尽量让数据保留原有的分布特征**

**Normalization**

**为什么标准化能够加速收敛？**

假设 U = Wx+b Z = F(U) x为输入，F为激活函数。那么随着网络层数的加深分布逐渐发生变动，导致整体分布逐渐往激活函数的饱和区间移动，从而反向传播时底层出现梯度消失，也就是收敛越来越慢的原因。

而Normalization则是把**分布强行拉回到均值为0方差为1的标准正态分布**，使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，避免梯度消失问题产生，加速收敛

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/ICS_1.png)

上图为标准正态分布的图形，可以看到，在一个标准差范围，有68%的概率x其值落在[-1,1]的范围内；在两个标准差范围，有95%的概率x其值落在了[-2,2]的范围内，假如这就是需要进行激活的分布，激活函数为sigmoid，如下：

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/ICS_2.png)

可以看到，在[-2, 2]的范围内，即是标准正态分布两个标注差范围内，在sigmoid函数中为线性变换区域，**微小的变化就能得到很大的改变，也即是梯度比较大**。如果不经过变换，存在一个均值为-6，方差为1的分布，对应到激活函数中就是[-8, -4]的区域，**这已经是饱和区了，这也就是所谓的梯度消失**。**所以标准化其实就是把大部分激活的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程**。

然而，如果使用标准化，那就相当于把非线性激活函数替换成线性函数了。那么使用非线性激活的意义在哪里呢，多层线性网络跟一层线性网络是等价的，也就是网络的表达能力下降了。

为了保证非线性表达能力，后面又对此打了个补丁，对变换后的满足均值为0方差为1的x进行了scale加上shift操作，形成类似y=scale∗x+shift这种形式，参数通过训练进行学习，把标准正态分布左移或者右移一点，并且长胖一点或者变瘦一点，将分布从线性区往非线性区稍微移动，希望找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力，又能够享受线性区较大的下降梯度。

下面是Normalization的模版形式(通用公式)：

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/norm_1.png)

**Batch Normalization —— 纵向规范化**

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/norm_2.png)

Batch Normalization 于2015年由 Google 提出，开 Normalization 之先河。其规范化针对单个神经元进行，利用网络训练时一个 mini-batch 的数据来计算该神经元 xi 的均值和方差,因而称为 Batch Normalization。

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/norm_3.png)

其中 M 是 mini-batch 的大小。

按上图所示，相对于一层神经元的水平排列，BN 可以看做一种纵向的规范化。由于 BN 是针对单个维度定义的，因此标准公式中的计算均为 element-wise 的。

BN 独立地规范化每一个输入维度 xi ，但规范化的参数是一个 mini-batch 的一阶统计量和二阶统计量。这就要求 每一个 mini-batch 的统计量是整体统计量的近似估计，或者说每一个 mini-batch 彼此之间，以及和整体数据，都应该是近似同分布的。分布差距较小的 mini-batch 可以看做是为规范化操作和模型训练引入了噪声，可以增加模型的鲁棒性；但如果每个 mini-batch的原始分布差别很大，那么不同 mini-batch 的数据将会进行不一样的数据变换，这就增加了模型训练的难度。因此，**BN 比较适用的场景是：每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle. 否则效果会差很多**。另外，由于 BN 需要在运行过程中统计每个 mini-batch 的一阶统计量和二阶统计量。因此不适用于 动态的网络结构 和 RNN 网络。需每一层进行标准化，同时还需要保存统计量，相对来说其内存占用较大。

**Layer Normalization —— 横向规范化**

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/norm_4.png)

如上，LN根据单个样本计算该层的平均值及方差，之后用同一个再平移及缩放因子规范各位维度的输入

![avatar](https://github.com/coderGray1296/code/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0/pictures/norm_5.png)

如上，LN针对单个样本进行计算，避免了batch分布的影响，同时不需要保存每层的统计量，节省了存储空间。但是需要注意的是，相对BN标准化的是单一维度，LN是对所有维度同时进行标准化，假如各个维度表示的特征的纲量不一致（比如颜色和大小），那么会导致模型的表达能力下降。
