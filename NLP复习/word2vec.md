# word2vec
谷歌2013年提出的Word2Vec是目前最常用的词嵌入模型之一。Word2Vec实际是一种浅层的神经网络模型，它有两种网络结构，**分别是CBOW（Continues Bag of Words）连续词袋和Skip-gram。**

#### 1.什么是word2vec
举个简单例子，判断一个词的词性，是动词还是名词。用机器学习的思路，我们有一系列样本(x,y)，这里 x 是词语，y 是它们的词性，我们要构建 f(x)->y 的映射，但这里的数学模型 f（比如神经网络、SVM）只接受数值型输入，而 NLP 里的词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec，就是词嵌入（ word embedding) 的一种

词向量其实是将词映射到一个语义空间，得到的向量。而word2vec是借用神经网络的方式实现的。

大部分的有监督机器学习模型，都可以归结为：f(x) -> y

在 NLP 中，把 x 看做一个句子里的一个词语，y 是这个词语的上下文词语，那么这里的 f，便是 NLP 中经常出现的『语言模型』（language model），这个模型的目的，就是判断 (x,y) 这个样本，是否符合自然语言的法则，更通俗点说就是：词语x和词语y放在一起，是不是人话。

Word2vec 正是来源于这个思想，**但它的最终目的，不是要把 f 训练得多么完美，而是只关心模型训练完后的副产物——模型参数（这里特指神经网络的权重），并将这些参数，作为输入 x 的某种向量化的表示，这个向量便叫做——词向量**

#### 2.Skip-gram 和 CBOW
根据我们上面提到的语言模型
- 如果是用一个词作为输入，来预测它的周围的上下文词，那这个模型就叫做 Skip-gram 模型
- 如果是用一个词语的上下文词来预测这个词语本身，那么就叫做这个模型为 CBOW 模型

##### 2.1 Skip-gram 和 CBOW 的简单情形
我们先来看个最简单的例子。上面说到， y 是 x 的上下文，所以 y 只取上下文里一个词语的时候，语言模型就变成：
```
用当前词 x 预测它的下一个词 y
```
但如上面所说，一般的数学模型只接受数值型输入，这里的 x 该怎么表示呢？ 显然不能用 Word2vec，因为这是我们训练完模型的产物，现在我们想要的是 x 的一个原始输入形式。
```
答案是：One-hot编码
```
举个例子，假设全世界所有的词语总共有 V 个，这 V 个词语有自己的先后顺序，假设『吴彦祖』这个词是第1个词，『我』这个单词是第2个词，那么『吴彦祖』就可以表示为一个 V 维全零向量、把第1个位置的0变成1，而『我』同样表示为 V 维全零向量、把第2个位置的0变成1。这样，每个词语都可以找到属于自己的唯一表示。

OK，那我们接下来就可以看看 Skip-gram 的网络结构了，x 就是上面提到的 one-hot encoder 形式的输入，y 是在这 V 个词上输出的概率，我们希望跟真实的 y 的 one-hot encoder 一样。

![avatar](https://github.com/coderGray1296/code/blob/master/NLP%E5%A4%8D%E4%B9%A0/pictures/word2vec_1.png)

首先说明一点：**隐层的激活函数其实是线性的**，相当于没做任何处理（这也是 Word2vec 简化之前语言模型的独到之处），我们要训练这个神经网络，用反向传播算法，本质上是链式求导，在此不展开说明了

当模型训练完后，最后得到的其实是**神经网络的权重**，比如现在输入一个 x 的 one-hot encoder: [1,0,0,…,0]，对应刚说的那个词语『吴彦祖』，则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，这些权重的个数，跟隐含层节点数是一致的，从而这些权重组成一个向量 vx 来表示x，而因为每个词语的 one-hot encoder 里面 1 的位置是不同的，所以，这个向量 vx 就可以用来唯一表示 x。

需要提到一点的是，这个词向量的维度（**与隐含层节点数一致**）一般情况下要远远小于词语总数 V 的大小，所以 Word2vec 本质上是一种**降维**操作——把词语从 one-hot encoder 形式的表示降维到 Word2vec 形式的表示。

##### 2.2 Skip-gram 更一般的情形
上面讨论的是最简单情形，即 y 只有一个词，当 y 有多个词时，网络结构如下：

![avatar](https://github.com/coderGray1296/code/blob/master/NLP%E5%A4%8D%E4%B9%A0/pictures/word2vec_2.png)

**可以看成是 单个x->单个y 模型的并联，cost function 是单个 cost function 的累加（取log之后）**

##### 2.3 CBOW 更一般的情况
跟 Skip-gram 相似，只不过: **Skip-gram 是预测一个词的上下文，而 CBOW 是用上下文预测这个词**

![avatar](https://github.com/coderGray1296/code/blob/master/NLP%E5%A4%8D%E4%B9%A0/pictures/word2vec_3.jpg)

更 Skip-gram 的模型并联不同，这里是输入变成了多个单词，所以要对输入处理下（**一般是求和然后平均**），输出的 cost function 不变

#### 3.Skip-gram 和 CBOW的训练过程及损失函数
[CBOW](https://blog.csdn.net/u010665216/article/details/78724856)
[Skip-gram](https://blog.csdn.net/u010665216/article/details/78721354)
