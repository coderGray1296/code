# 海量数据总结
海量数据问题的解决方法可以大致分为几类

## Hash(常与归并一起用)
对于所有的数据不能直接存入内存，可以利用hash()函数将源数据映射到某一区间内，从而划分成若干个小文件，分治处理。同时相同的源数据hash值一定一样，因此会映射到同一个文件中。
然后在每个小文件中可以利用堆或者hash_map进行排序，然后多个文件归并进行

e.g. 海量日志数据，提取出某日访问百度次数最多的那个IP。

## Bit-Map
可进行数据的快速查找，判重，删除，一般来说数据范围是int的10倍以下

e.g. 2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。

## 堆
海量数据前n大，并且n比较小，堆可以放入内存

最大堆求前n小，最小堆求前n大。方法，比如求前n小，我们比较当前元素与最大堆里的最大元素，如果它小于最大元素，则应该替换那个最大元素。这样最后得到的n个元素就是最小的n个。适合大数据量，求前n小，n的大小比较小的情况，这样可以扫描一遍即可得到所有的前n元素，效率很高。

扩展：双堆，一个最大堆与一个最小堆结合，可以用来维护中位数。

e.g. 100w个数中找最大的前100个数。

## 双层桶划分(二分的思想)
e.g. 5亿个int找它们的中位数。

## Trie树
数据量大，重复多，但是数据种类小可以放入内存

e.g. 寻找热门查询：查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个，每个不超过255字节。

# 经典问题分析
上千万or亿数据（有重复），统计其中出现次数最多的前N个数据,分两种情况：**可一次读入内存，不可一次读入**。

可用思路：trie树+堆，数据库索引，划分子集分别统计，hash，分布式计算，近似统计，外排序

所谓的是否能一次读入内存，实际上应该指去除重复后的数据量。如果去重后数据可以放入内存，我们可以为数据建立字典，比如通过 map，hashmap，trie，然后直接进行统计即可。当然在更新每条数据的出现次数的时候，我们可以利用一个堆来维护出现次数最多的前N个数据，当然这样导致维护次数增加，不如完全统计后在求前N大效率高。

如果数据无法放入内存。一方面我们可以考虑上面的字典方法能否被改进以适应这种情形，可以做的改变就是将字典存放到硬盘上，而不是内存，这可以参考数据库的存储方法。

当然还有更好的方法，就是可以采用分布式计算，基本上就是map-reduce过程，首先可以根据数据值或者把数据hash(md5)后的值，将数据按照范围划分到不同的机子，最好可以让数据划分后可以一次读入内存，这样不同的机子负责处理各种的数值范围，实际上就是map。得到结果后，各个机子只需拿出各自的出现次数最多的前N个数据，然后汇总，选出所有的数据中出现次数最多的前N个数据，这实际上就是reduce过程。
